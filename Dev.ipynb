{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connection √† l'API de Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "google_API_TOKEN = os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(\"Sing a ballad of LangChain.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "import datetime\n",
    "from discord import Poll\n",
    "\n",
    "@tool\n",
    "def create_poll(\n",
    "    question: str,\n",
    "    answers: str,\n",
    "    duration: int = 24,\n",
    "    answers_emoji: str = None,\n",
    "    multiple: bool =True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Commande pour cr√©er un sondage avec r√©actions.\n",
    "    \"\"\"\n",
    "    answers = answers.split()\n",
    "    answers_emoji = answers_emoji.split()\n",
    "    \n",
    "    poll = Poll(question,datetime.timedelta(hours=duration),multiple=multiple)\n",
    "\n",
    "    for answer, emoji in zip(answers, answers_emoji):\n",
    "        print(f'Answers :{answer},{emoji}')\n",
    "        poll.add_answer(text=answer, emoji=emoji)\n",
    "\n",
    "    return Poll\n",
    "\n",
    "\n",
    "tools = [create_poll]\n",
    "\n",
    "graph = create_react_agent(llm, tools=tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stream(stream):\n",
    "    for s in stream:\n",
    "        message = s[\"messages\"][-1]\n",
    "        if isinstance(message, tuple):\n",
    "            print(message)\n",
    "        else:\n",
    "            message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Je veux que tu cr√©es un sondage simple et attractif sur les fruits. La question principale doit √™tre : 'Quel est votre fruit pr√©f√©r√© ?'. Propose des choix de r√©ponse vari√©s et populaires, comme :Pomme üçè, Banane üçå, Orange üçä,Fraiseüçì, Mangue ü•≠, Raisin üçá\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  create_poll (7c565f9f-0024-4df9-9e36-0060ac0d6928)\n",
      " Call ID: 7c565f9f-0024-4df9-9e36-0060ac0d6928\n",
      "  Args:\n",
      "    answers: Pomme üçè, Banane üçå, Orange üçä, Fraise üçì, Mangue ü•≠, Raisin üçá\n",
      "    question: Quel est votre fruit pr√©f√©r√© ?\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: create_poll\n",
      "\n",
      "Error: AttributeError(\"'NoneType' object has no attribute 'split'\")\n",
      " Please fix your mistakes.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Il y a eu une erreur lors de la cr√©ation du sondage. Je vais r√©essayer en m'assurant que les arguments sont correctement format√©s.\n",
      "Tool Calls:\n",
      "  create_poll (595c9e48-aa87-49bc-ba3b-e1780b16e6f8)\n",
      " Call ID: 595c9e48-aa87-49bc-ba3b-e1780b16e6f8\n",
      "  Args:\n",
      "    answers: Pomme üçè, Banane üçå, Orange üçä, Fraise üçì, Mangue ü•≠, Raisin üçá\n",
      "    question: Quel est votre fruit pr√©f√©r√© ?\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: create_poll\n",
      "\n",
      "Error: AttributeError(\"'NoneType' object has no attribute 'split'\")\n",
      " Please fix your mistakes.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "It seems like I'm encountering an error related to the way the answers are being processed. I'll try creating the poll again, but this time I'll ensure the answers are in the correct format.\n",
      "Tool Calls:\n",
      "  create_poll (dd010436-d747-43aa-aec2-a2ffe5acb6c9)\n",
      " Call ID: dd010436-d747-43aa-aec2-a2ffe5acb6c9\n",
      "  Args:\n",
      "    answers_emoji: üçè,üçå,üçä,üçì,ü•≠,üçá\n",
      "    answers: Pomme,Banane,Orange,Fraise,Mangue,Raisin\n",
      "    question: Quel est votre fruit pr√©f√©r√© ?\n",
      "Answers :Pomme,Banane,Orange,Fraise,Mangue,Raisin,üçè,üçå,üçä,üçì,ü•≠,üçá\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: create_poll\n",
      "\n",
      "<class 'discord.poll.Poll'>\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "D'accord, j'ai cr√©√© un sondage avec la question \"Quel est votre fruit pr√©f√©r√© ?\" et les options de r√©ponse suivantes : Pomme üçè, Banane üçå, Orange üçä, Fraise üçì, Mangue ü•≠, Raisin üçá.\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"messages\": [(\"user\", \"Je veux que tu cr√©es un sondage simple et attractif sur les fruits. La question principale doit √™tre : 'Quel est votre fruit pr√©f√©r√© ?'. Propose des choix de r√©ponse vari√©s et populaires, comme :Pomme üçè, Banane üçå, Orange üçä,Fraiseüçì, Mangue ü•≠, Raisin üçá\")]}\n",
    "print_stream(graph.stream(inputs, stream_mode=\"values\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "# this is the state schema used by the prebuilt create_react_agent we'll be using below\n",
    "from langgraph.prebuilt.chat_agent_executor import AgentState\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "class State(AgentState):\n",
    "    docs: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import InjectedState\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_context(question: str, state: Annotated[dict, InjectedState]):\n",
    "    \"\"\"Get relevant context for answering the question.\"\"\"\n",
    "    return \"\\n\\n\".join(doc for doc in state[\"docs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolNode, create_react_agent\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "tools = [get_context]\n",
    "\n",
    "# ToolNode will automatically take care of injecting state into tools\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "checkpointer = MemorySaver()\n",
    "graph = create_react_agent(llm, tools, state_schema=State, checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"FooBar company just raised 1 Billion dollars!\",\n",
    "    \"FooBar company was founded in 2019\",\n",
    "]\n",
    "\n",
    "inputs = {\n",
    "    \"messages\": [{\"type\": \"user\", \"content\": \"what's the latest news about FooBar\"}],\n",
    "    \"docs\": docs,\n",
    "}\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for chunk in graph.stream(inputs, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def prochain_mercredi(date_reference: datetime = None) -> datetime:\n",
    "    \"\"\"\n",
    "    Retourne la date du prochain mercredi √† partir de la date de r√©f√©rence donn√©e.\n",
    "    Si aucune date de r√©f√©rence n'est fournie, utilise la date actuelle.\n",
    "    \"\"\"\n",
    "    date_reference = date_reference.date() if date_reference else datetime.today().date()\n",
    "    \n",
    "    # Calcul des jours √† ajouter pour atteindre mercredi (mercredi = 2)\n",
    "    jours_a_ajouter = (2 - date_reference.weekday()) % 7 or 7  # Assure le mercredi suivant\n",
    "    \n",
    "    return datetime.combine(date_reference + timedelta(days=jours_a_ajouter), datetime.min.time())\n",
    "\n",
    "def prochain_mercredi(date_reference: datetime = None) -> datetime:\n",
    "    \"\"\"\n",
    "    Retourne la date du prochain mercredi √† 21h15 √† partir de la date de r√©f√©rence donn√©e.\n",
    "    Si aucune date de r√©f√©rence n'est fournie, utilise la date actuelle.\n",
    "    \n",
    "    :param date_reference: Une date de r√©f√©rence optionnelle (datetime).\n",
    "    :return: Un objet datetime repr√©sentant le prochain mercredi √† 21h15.\n",
    "    \"\"\"\n",
    "    date_reference = date_reference if date_reference else datetime.today()\n",
    "    \n",
    "    # Calcul des jours √† ajouter pour atteindre mercredi (mercredi = 2)\n",
    "    jours_a_ajouter = (2 - date_reference.weekday()) % 7 or 7  # Assure le mercredi suivant\n",
    "    \n",
    "    return datetime.combine(date_reference.date() + timedelta(days=jours_a_ajouter), datetime.min.time().replace(hour=21, minute=15))\n",
    "\n",
    "def discord_timestamps(date: datetime, format: str = 'f') -> str:\n",
    "    \"\"\"\n",
    "    G√©n√®re un timestamp Discord √† partir d'une date.\n",
    "    \n",
    "    :param date: Un objet datetime\n",
    "    :param format: Format du timestamp Discord ('F', 'f', 'D', 'd', 'T', 't', 'R')\n",
    "    :return: Une cha√Æne de caract√®res compatible avec Discord\n",
    "    \"\"\"\n",
    "    accepted_formats = {'F', 'f', 'D', 'd', 'T', 't', 'R'}\n",
    "    \n",
    "    if format not in accepted_formats:\n",
    "        raise ValueError(f\"Format non pris en charge. Formats accept√©s : {', '.join(accepted_formats)}\")\n",
    "    \n",
    "    timestamp = int(date.timestamp())  # Convertir en timestamp Unix\n",
    "    return f'<t:{timestamp}:{format}>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<t:1749064500:f>'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discord_timestamps(prochain_mercredi())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Hey! N'oubliez pas de voter pour le film de la watchparty ! La soir√©e film aura lieu <t:1749064500:f> (<t:1749064500:R>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"## Hey! N'oubliez pas de voter pour le film de la watchparty ! La soir√©e film aura lieu {discord_timestamps(prochain_mercredi())} ({discord_timestamps(prochain_mercredi(), format='R')})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
